---
title: 'Bayesian Analysis of Crack-Seal Veins'
author: 'Daniel Pliego'
date: "18 April, 2025"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#if you do not have the package, type install.packages("name_of_the_package")
library(knitr)
library(tseries)
library(readxl)
library(dplyr)
library(rstan)
library(bayesplot)
library(ggplot2)

source("Data_clean.R")
```





```{r get data,  echo=FALSE, message=FALSE, warning=FALSE}
# This here is to get the dataset, I am going to not use samples 4 and 6
# as they are very small.
data <- get_dataset()

data <- data %>%
  # Remove rows where Sample is 4 or 6
  filter(!(Sample %in% c(4,6))) %>%
  #re label sample 5 to 4
  mutate(Sample = ifelse(Sample == 5, 4, Sample))
```



GitHub repo: https://github.com/pliego02/STAT-447-Project


## Introduction

The idea of this project is to do Bayesian analysis on a data set of crack seal veins provided by Dr. Matthew Tarling from the Geological Sciences department at UBC. This data set consists of thickness measurements of consecutive layers or bands measured in micrometers from four different samples of "crack-seal veins".

Crack-seal veins are created beneath Earth's surface through repeated fractures and sealing of these fractures. Geological processes cause rocks to fracture, but due to the high pressure in these environments, the cracks can't stay open. Instead, they are filled with fluids that precipitate crystals, creating distinct layers or bands. Each band represents a separate fracture and sealing event. Studying crack-seal veins is interesting because they preserve a record of tectonic activity over time, which can help us better understand how faults behave and then give us more insight into the behavior of earthquakes.

There have been suggestions that consecutive bands in crack-seal veins do not exhibit correlation, and that their thicknesses follow an exponential distribution (Renard et al., 2005). However, using Monte Carlo simulations, Williams et al. (2022) showed that it is unlikely these events follow an exponential distribution, based on the coefficient of variation (COV), defined as the standard deviation divided by the mean. The aim of this project is therefore to identify better fitting distributions to model the thickness of crack-seal veins, and to use these improved models to fit an AR(1) (autoregressive of order 1) model in order to examine potential correlation between consecutive bands.

Rank and trace plots have been explored for every parameter from every model on this project and they all appear appropriate in that the trace plots of different chains behave similarly and the rank plots appear uniform. However due to spacing none of these plot will be provided on this file. There is a file in the repo which contains all of this plots. I also want to mention that ChatGpt was used in the project to help debug the posterior predictive check plots and the indexes for the AR(1) model


## Exponential Likelihood model

The first model I will fit is one with an exponential likelihood. Based on the findings of Williams et al. (2022), we should expect to obtain similar results, where the observed coefficient of variation (COV) falls outside the 95% confidence interval computed using simulated data from the posterior distribution. This would indicate that the posterior does not accurately capture the variability observed in the real data, suggesting that the exponential model may not be an appropriate fit.


The model is of the form:
\begingroup\footnotesize
$$
\begin{aligned}
\lambda_i &\sim \mathrm{Uniform}(0,1) \\
X_n \mid \lambda_i  &\sim \mathrm{Exp}(\lambda_i)
\end{aligned}
$$
\endgroup
Where $X_n$ is the thickness observed thickness and $\lambda_i$ is the parameter for each sample i. This will make the stan model generate lambda's for each one of the samples



Posterior predictive check for each sample.

The solid red histogram shows the actual observed thicknesses for each sample. Overlaid in translucent blue are 100 histograms of simulated thicknesses each one generated by plugging a draw of the modelâ€™s parameters (from the joint posterior) into the likelihood. The legend reports the observed coefficient of variation (COV) alongside its 95% posterior predictive interval.

```{r fit exponential model,  echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
stan_data <- list(
  N = nrow(data),          
  x = data$Thickness,       
  sample = data$Sample      
)

# Compile and sample
exp_fit <- stan(
  file = "exponential_model.stan",
  data = stan_data,
  iter = 10000,
  chains = 1,
  set.seed(2002)
)
```

```{r exponential ppc,  echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5, fig.width=6.5}
set.seed(2002)
par(mfrow = c(2,2))

exp_fit <- extract(exp_fit)
n_iter <- nrow(exp_fit$cov_rep)
n_rep   <- 100                

for (s in 1:4) {
  # indices & observed data
  idx       <- which(stan_data$sample == s)
  data_obs  <- stan_data$x[idx]
  N_obs     <- length(idx)

  # build a matrix of posterior reps
  rep_mat   <- t(exp_fit$x_rep[, idx])

  # compute observed CoV
  cov_obs   <- sd(data_obs) / mean(data_obs)

  # compute 95% CI of posterior CoV
  cov_ci    <- quantile(exp_fit$cov_rep[, s], probs = c(0.025, 0.975))

  # plot observed histogram
  hist(data_obs,
       breaks = 30,
       prob   = TRUE,
       main   = paste("Posterior Predictive Check Sample", s),
       xlab   = "Thickness",
       col    = rgb(1,0,0,0.5),
       border = "white")

  # overlay a random subset of posterior replicates
  draw_idxs <- sample(1:n_iter, n_rep)
  for (i in draw_idxs) {
    hist(rep_mat[, i],
         breaks = 30,
         prob   = TRUE,
         add    = TRUE,
         col    = rgb(0,0,1,0.01),
         border = NA)
  }

  # add CoV legend
  legend("topright", bty = "n",
         legend = c(
           sprintf("CV obs = %.3f", cov_obs),
           sprintf("95%%  CI = [%.3f, %.3f]",
                   cov_ci[1], cov_ci[2])
         ))
}
```


As expected in all of the samples the observed COV is far away from the 95% interval. We can also see that the posterior distribution with an exponential likelihood for the thickness does not fit the data very well. This is expected and is what was show on the paper.

As said on the introduction I wont provide trace or rank plots on this file. However, they all looked appropriate. 

## Model 2, Gamma model

We have seen that the exponential model does not fit the data properly this is likely due to the assumption of equal variance and mean from the exponential distribution. I will implement a Gamma model to relax that assumption. 

I will also implement a hierarchical model to help inform the prior.

The model is of the form:

\begingroup\footnotesize
$$
\begin{aligned}
\mu_{\alpha} &\sim {Exp}(1) \\
\theta_{\alpha} &\sim {Exp}(1) \\
\mu_{\beta} &\sim {Exp}(1) \\
\theta_{\beta} &\sim {Exp}(1) \\
\alpha_i \mid \mu_{\alpha}, \theta_{\alpha} &\sim {Gamma}(\mu_{\alpha}, \theta_{\alpha}) \\
\beta_i \mid \mu_{\beta}, \theta_{\beta} &\sim {Gamma}(\mu_{\beta}, \theta_{\beta}) \\
X_n \mid \alpha_i, \beta_i &\sim {Gamma}(\alpha_i, \beta_i)
\end{aligned}
$$
\endgroup



```{r fit gamma model,  echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
stan_data <- list(
  N = nrow(data),          
  x = data$Thickness,       
  sample = data$Sample      
)

# Compile and sample
gamma_fit <- stan(
  file = "gamma_model.stan",
  data = stan_data,
  iter = 10000,
  chains = 1,
  set.seed(2002)
)
```


I will provide posterior predictive checks, however due to the hierarchical model, I will perform mixed predictive replication for hierarchical models which are showed how to perform on the stan users guide. This are posterior predictive checks with the difference that the hyper parameters remain fixed 


```{r gamma ppc,  echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5, fig.width=6.5}
set.seed(2002)
par(mfrow = c(2,2))
# I will make the CoV calculation here on R rather than on stan
# For simplicity of the stan file. 

gamma_fit <- extract(gamma_fit)


# Pre compute observed CoV per sample, ChatGpT helped made this function
cov_obs <- tapply(stan_data$x, stan_data$sample,
                  function(xx) sd(xx)/mean(xx))




n_iter <- nrow(gamma_fit$x_rep)
# how many posterior predictive histograms to overlay
n_rep   <- 100  


for (s in 1:4) {
  idx      <- which(stan_data$sample == s)
  data_obs <- stan_data$x[idx]
  # make a matrix of rows
  rep_mat  <- t(gamma_fit$x_rep[, idx])

  # ChatGpt also helped debug this
  cov_rep_samps <- apply(rep_mat, 2, function(xx) sd(xx)/mean(xx))
  cov_ci        <- quantile(cov_rep_samps, c(0.025, 0.975))

  # plot observed histogram
  hist(data_obs,
       breaks = 30,
       prob   = TRUE,
       main   = paste("Sample", s),
       xlab   = "x",
       col    = rgb(1, 0, 0, 0.5),
       border = "white")

  # overlay a random subset of the posterior predictive histograms
  draw_idxs <- sample(n_iter, n_rep)
  for (i in draw_idxs) {
    hist(rep_mat[, i],
         breaks = 30,
         prob   = TRUE,
         add    = TRUE,
         col    = rgb(0, 0, 1, 0.01),
         border = NA)
  }

  # add CoV legend
  legend("topright", bty = "n",
         legend = c(
           sprintf("CV obs = %.3f", cov_obs[s]),
           sprintf("95%% CI = [%.3f, %.3f]",
                   cov_ci[1], cov_ci[2])
         ))
}

```

Looking at the plots, the hierarchical gamma model appears to fit the data much better, additionally the CoV measurement is now inside the 95% CI provided by the posterior distribution in all samples but sample 2. This is already a huge improvement compared to the exponential model.

The gamma likelihood seems to miss where the peak of the observed data is. However it seems to model well the right tail of the distribution.



## Model 3, Normal model

Finally I want to test a normal distribution model. The normal distribution does not have a support on the positive real numbers as the gamma and exponential distribution do. However the thickness values are on average a few standard deviations away from 0. Which makes it so that fitting a normal distribution should have little to no predictions which are negative. I will also use a hierarchical model to help inform the prior. 


model:
\begingroup\footnotesize
$$
\begin{aligned}
\alpha_{\mu} &\sim {Exp}(1) \\
\alpha_{\theta}  &\sim {Exp}(1) \\
\beta_{\mu} &\sim {Exp}(1) \\
\beta_{\theta}  &\sim {Exp}(1) \\
\mu_i \mid \alpha_{\mu}, \beta_{\mu} &\sim {Gamma}(\alpha_{\mu},\beta_{\mu}) \\
\theta_i  \mid \alpha_{\theta}, \beta_{\theta} &\sim {Gamma}(\alpha_{\theta}, \beta_{\theta}) \\
X_n \mid \mu_i , \theta_i &\sim {Normal}(\mu_i , \theta_i)
\end{aligned}
$$
\endgroup



```{r fit normal model, echo=FALSE, message=FALSE, warning=FALSE,results = 'hide'}
stan_data <- list(
  N = nrow(data),          
  x = data$Thickness,       
  sample = data$Sample      
)

# Compile and sample
normal_fit <- stan(
  file = "normal_model.stan",
  data = stan_data,
  iter = 10000,
  chains = 1,
  set.seed(2002)
)
```





```{r normal ppc, echo=FALSE, message=FALSE, warning=FALSE,  fig.height=4.5, fig.width=6.5}
set.seed(2002)
par(mfrow = c(2, 2))  


normal_fit <- extract(normal_fit)


# Pre compute observed CoV per sample, ChatGpT helped made this function
cov_obs <- tapply(stan_data$x, stan_data$sample,
                  function(xx) sd(xx)/mean(xx))




n_iter <- nrow(normal_fit$x_rep)
# how many posterior predictive histograms to overlay
n_rep   <- 100  


for (s in 1:4) {
  idx      <- which(stan_data$sample == s)
  data_obs <- stan_data$x[idx]
  # make a matrix of rows
  rep_mat  <- t(normal_fit$x_rep[, idx])

  # ChatGpt also helped debug this
  cov_rep_samps <- apply(rep_mat, 2, function(xx) sd(xx)/mean(xx))
  cov_ci        <- quantile(cov_rep_samps, c(0.025, 0.975))

  # plot observed histogram
  hist(data_obs,
       breaks = 30,
       prob   = TRUE,
       main   = paste("Sample", s),
       xlab   = "x",
       col    = rgb(1, 0, 0, 0.5),
       border = "white")

  # overlay a random subset of the posterior predictive histograms
  draw_idxs <- sample(n_iter, n_rep)
  for (i in draw_idxs) {
    hist(rep_mat[, i],
         breaks = 30,
         prob   = TRUE,
         add    = TRUE,
         col    = rgb(0, 0, 1, 0.01),
         border = NA)
  }

  # add CoV legend
  legend("topright", bty = "n",
         legend = c(
           sprintf("CV obs = %.3f", cov_obs[s]),
           sprintf("95%% CI = [%.3f, %.3f]",
                   cov_ci[1], cov_ci[2])
         ))
}
```

By looking at the plots I believe the normal likelihood models the observed data much better. The blue shaded simulated samples appear to be much closer to the observed data. Additionally we can see that now all observed COV are well inside the 95% confidence interval made by the simulated data from the posterior.


The normal likelihood seems to perform the better out of the three distributions even though it has an incorrect support compared to the other 2 distributions.

### AR(1) Model Using a Normal Distribution

I will model the AR(1) (autoregressive model of order 1) using a normal distribution. I choose to use a normal distribution as it is the one that best fits the data from the models above. I will also make the model hierarchical to help inform the priors. 

I will make 2 different models. One will have the AR(1) coefficient shared between all 4 samples and the second one will fit a different AR(1) coefficient to each of the different samples. I will only provide the posterior predictive check plots for the model with a shared AR(1) parameter.

model with shared AR(1) parameter $\phi$ between samples:

\begingroup\footnotesize
$$
\begin{aligned}
\alpha_{\mu} &\sim {Exp}(1) \\
\beta_{\mu}   &\sim {Exp}(1) \\
\alpha_{\theta} &\sim {Exp}(1) \\
\beta_{\theta}  &\sim {Exp}(1) \\
\mu_i    &\sim {Gamma}(\alpha_{\mu}, \beta_{\mu}) \\
\theta_i &\sim {Gamma}(\alpha_{\theta}, \beta_{\theta}) \\
\phi   &\sim {Normal}(0, 0.5)\\
X_{f_i} &\sim {Normal}(\mu_i, \theta_i) 
   \quad\text{(first observation)} \\
X_n     &\sim {Normal}(\mu_i + \phi\,(X_{n-1} - \mu_i),\;\theta_i)
\end{aligned}
$$
\endgroup

The model with parameter AR(1) $\phi_i$ for each sample is only different in that it has:


\begingroup\footnotesize
$$
\begin{aligned}
\phi_i   &\sim {Normal}(0, 0.5)   \quad\text{(for sample i)}  \\
X_n     &\sim {Normal}(\mu_i + \phi_i\,(X_{n-1} - \mu_i),\;\theta_i)
\end{aligned}
$$
\endgroup



```{r add index to data, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
# This R chunk deals with including an index on the data, this makes it easier
# to handle in stan when fitting an AR model

N <- nrow(data)
G <- length(unique(data$Sample))
y <- data$Thickness
group <- data$Sample


# find the first and last row for each Sample
# ChatGpt was used to help debug this and how to handle the indexes properly
# in stan
lims <- data %>%
  mutate(row_id = row_number()) %>%
    group_by(Sample) %>%
  summarize(first = min(row_id),
            last  = max(row_id))


# make the stan data
stan_data <- list(
  N         = N,
  G         = G,
  y         = y,
  group     = group,
  first_obs = lims$first,
  last_obs  = lims$last
)

```



```{r fit AR model, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
# Fit 2 chains to make the plots
ar_model_fit_obj <- stan(
  file = "normal_AR(1)_model.stan",
  data = stan_data,
  iter = 10000,
  chains = 1,
  set.seed(2002)
)
```

```{r fit AR model 4 params, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
# Fit 2 chains to make the plots
ar_model_fit_obj_4_params <- stan(
  file = "normal_AR(1)_model_4_params.stan",
  data = stan_data,
  iter = 10000,
  chains = 1,
  set.seed(2002)
)
```

```{r AR ppc plots, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5, fig.width=6.5}
set.seed(2002)
par(mfrow = c(2, 2))   

# Extract posterior draws
ar_model_fit <- extract(ar_model_fit_obj)
y_rep    <- ar_model_fit$y_rep   

# Observed data and grouping
y_obs <- stan_data$y
grp   <- stan_data$group
G     <- length(unique(grp))

# Compute observed CoV per group
cov_obs <- tapply(y_obs, grp, function(yy) sd(yy) / mean(yy))

n_iter <- nrow(y_rep)  
n_rep  <- 100        

for (s in 1:G) {
 
  idx      <- which(grp == s)
  data_obs <- y_obs[idx]
  
  
  rep_mat  <- t(y_rep[, idx])
  
  # CoV for each posterior replicate
  cov_rep_samps <- apply(rep_mat, 2, function(xx) sd(xx) / mean(xx))
  cov_ci        <- quantile(cov_rep_samps, c(0.025, 0.975))
  
  # Plot observed histogram
  hist(data_obs,
       breaks    = 30,
       prob      = TRUE,
       main      = paste("Group", s),
       xlab      = "Thickness",
       col       = rgb(1, 0, 0, 0.5),
       border    = "white")
  
  # Overlay a random subset of posterior histograms
  draw_idxs <- sample(n_iter, n_rep)
  for (i in draw_idxs) {
    hist(rep_mat[, i],
         breaks = 30,
         prob   = TRUE,
         add    = TRUE,
         col    = rgb(0, 0, 1, 0.01),
         border = NA)
  }
  
  # Add CoV legend
  legend("topright", bty = "n",
         legend = c(
           sprintf("CV obs = %.3f",      cov_obs[s]),
           sprintf("95%% CI = [%.3f, %.3f]",
                   cov_ci[1], cov_ci[2])
         ))
}
```

The posterior predictive checks all suggest the posterior seems appropriate.


For an AR(1) model like the one we fitted above, the auto correlation coefficient at lag 1 is equal to the AR coefficient. 
$$
\rho(1) = \phi
$$

Therefore the resulting posterior $\rho(1)$ values for each of the samples along with a 95% confidence interval are:


```{r phi-ci-table, echo=FALSE, message=FALSE, warning = FALSE}
lims <- lims %>%
  mutate(size = last - first + 1)

phi_summary <- summary(ar_model_fit_obj_4_params,
                       pars = paste0("phi[", 1:4, "]"))$summary

phi_df <- as.data.frame(phi_summary) %>%
  select(mean = mean, `2.5%` = `2.5%`, `97.5%` = `97.5%`) %>%
  mutate(Sample = 1:4)

# Join with sample sizes
phi_df <- left_join(phi_df, lims, by = "Sample")

#Compute significance bounds and determine if autocorrelation is significant
phi_df <- phi_df %>%
  mutate(
    bound = 2 / sqrt(size),
    significant = (`2.5%` > bound) | (`97.5%` < -bound)
  )

# kable table
phi_df %>%
  select(Sample, mean, `2.5%`, `97.5%`, bound, significant) %>%
  kable(digits = 3, caption = "95% Confidence Intervals and Autocorrelation Significance per Sample")
```

```{r phi-shared-significance, echo=FALSE ,message=FALSE, warning=FALSE}

#get N
lims <- lims %>% mutate(size = last - first + 1)
N_total <- sum(lims$size)

#Extract posterior summary for the shared phi
phi_sum <- summary(ar_model_fit_obj, pars = "phi")$summary
phi_df <- tibble(
  Sample = "all",
  mean      = phi_sum["phi", "mean"],
  `2.5%`    = phi_sum["phi", "2.5%"],
  `97.5%`   = phi_sum["phi", "97.5%"],
  size      = N_total
)

# Compute the bound
phi_df <- phi_df %>%
  mutate(
    bound       = 2 / sqrt(size),
    significant = (`2.5%` > bound) | (`97.5%` < -bound)
  )

# 4
phi_df %>%
  select(Sample, mean, `2.5%`, `97.5%`, size, bound, significant) %>%
  kable(digits = 3,
        caption = "95% Confidence Intervals and Autocorrelation Significance Shared")


```










Values outside the interval $(-\frac{2}{\sqrt{n}},\frac{2}{\sqrt{n}})$ are considered significant when determining whether the data deviates from a white noise process. In the table, "bound" refers to the value of $\frac{2}{\sqrt{n}}$ for each sample.

We can see that the model with a single AR coefficient shared across all samples suggests there is correlation between consecutive bands, since the posterior 95% interval lies entirely outside the bound. However, it's important to note that sample 2 is the only one out of the four that clearly showed signs of autocorrelation, and it also happens to be the largest sample as it contains about four times as many observations as the others. This suggests that sample 2 may indeed have some autocorrelation. However, due to the limited data and the individual results from each sample, I'm unsure whether all samples exhibit the same behavior.


## Results

As expected, and consistent with the findings of Williams et al. (2022), the exponential distribution does not model the thickness of crack-seal veins particularly well. In this project, I tested both gamma and normal distributions as alternatives, with the normal distribution providing the best fit among the three. Based on this result, I used a normal likelihood in an AR(1) model to explore the autocorrelation behavior between consecutive bands.

The analysis showed that sample 2 exhibited significant autocorrelation at lag 1, suggesting that there is a correlation in band thickness from one event to the next in that sample. When fitting an AR(1) model with a shared coefficient across all samples, the results indicated significant autocorrelation between consecutive bands overall. However, when examining each sample individually, it became clear that not all samples exhibit the same behavior. Due to the limited size of the dataset, I remain uncertain whether all samples follow the same correlation structure as sample 2.


## Discussion

This analysis of crack-seal veins is based on only four samples. To obtain more reliable results, the procedures outlined above should be tested on a larger number of samples, ideally with more observations per sample. In addition to expanding the dataset, I think it would be very interesting to apply Bayesian analysis of crack-seal veins in the frequency domain. This idea has been mentioned by Renard et al. (2005), but I believe it deserves further exploration using Bayesian methods.


## References
Posterior and prior predictive checks. (n.d.). Stan Docs. https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html

Renard, F., AndrÃ©ani, M., Boullier, A., & Labaume, P. (2005). Crack-seal patterns: records of uncorrelated stress release variations in crustal rocks. Geological Society London Special Publications, 243(1), 67â€“79. https://doi.org/10.1144/gsl.sp.2005.243.01.07

Williams, R. T., & Kirkpatrick, J. D. (2022). Are low-frequency earthquake moments area- or slip-limited? A
rock record examination. Geophysical Research Letters, 49, e2021GL095759. https://doi.org/10.1029/2021GL095759


































